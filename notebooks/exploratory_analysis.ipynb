{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGO Network Analysis - Exploratory Data Analysis\n",
    "\n",
    "This notebook provides examples of how to analyze the scraped data from Czech climate NGOs.\n",
    "\n",
    "## Contents\n",
    "1. Loading scraped data\n",
    "2. Link network analysis\n",
    "3. Document analysis\n",
    "4. Visualization\n",
    "5. Statistical summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directory\n",
    "data_dir = Path('../data')\n",
    "\n",
    "# List all NGOs that have been scraped\n",
    "ngo_dirs = list((data_dir / 'raw').glob('*'))\n",
    "print(f\"Found {len(ngo_dirs)} scraped NGOs:\")\n",
    "for ngo_dir in ngo_dirs:\n",
    "    print(f\"  - {ngo_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ngo_data(ngo_name):\n",
    "    \"\"\"\n",
    "    Load all data for a specific NGO.\n",
    "    Returns links, metadata, and session info.\n",
    "    \"\"\"\n",
    "    # Find the most recent scraping session\n",
    "    ngo_path = data_dir / 'raw' / ngo_name\n",
    "    sessions = sorted(ngo_path.glob('*'), reverse=True)\n",
    "    \n",
    "    if not sessions:\n",
    "        print(f\"No data found for {ngo_name}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    latest_session = sessions[0]\n",
    "    print(f\"Loading data from: {latest_session}\")\n",
    "    \n",
    "    # Load links\n",
    "    links_file = latest_session / 'links.json'\n",
    "    if links_file.exists():\n",
    "        with open(links_file, 'r', encoding='utf-8') as f:\n",
    "            links = json.load(f)\n",
    "    else:\n",
    "        links = []\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_file = latest_session / 'metadata.json'\n",
    "    if metadata_file.exists():\n",
    "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    # Count pages and documents\n",
    "    pages_dir = latest_session / 'pages'\n",
    "    docs_dir = latest_session / 'documents'\n",
    "    \n",
    "    n_pages = len(list(pages_dir.glob('*.html'))) if pages_dir.exists() else 0\n",
    "    n_docs = len(list(docs_dir.glob('*'))) if docs_dir.exists() else 0\n",
    "    \n",
    "    session_info = {\n",
    "        'ngo_name': ngo_name,\n",
    "        'session_date': latest_session.name,\n",
    "        'n_pages': n_pages,\n",
    "        'n_documents': n_docs,\n",
    "        'n_links': len(links)\n",
    "    }\n",
    "    \n",
    "    return links, metadata, session_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load data for a specific NGO\n",
    "ngo_name = 'Hnut√≠ DUHA'  # Change this to any NGO name\n",
    "links, metadata, session_info = load_ngo_data(ngo_name)\n",
    "\n",
    "if session_info:\n",
    "    print(\"\\nSession Summary:\")\n",
    "    print(f\"  NGO: {session_info['ngo_name']}\")\n",
    "    print(f\"  Session Date: {session_info['session_date']}\")\n",
    "    print(f\"  Pages Scraped: {session_info['n_pages']}\")\n",
    "    print(f\"  Documents Downloaded: {session_info['n_documents']}\")\n",
    "    print(f\"  Links Extracted: {session_info['n_links']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Link Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert links to DataFrame\n",
    "if links:\n",
    "    links_df = pd.DataFrame(links)\n",
    "    print(f\"Total links: {len(links_df)}\")\n",
    "    print(f\"\\nLink types:\")\n",
    "    print(links_df['link_type'].value_counts())\n",
    "    \n",
    "    # Display sample links\n",
    "    print(\"\\nSample links:\")\n",
    "    links_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze internal link structure\n",
    "if links:\n",
    "    internal_links = links_df[links_df['link_type'] == 'internal']\n",
    "    \n",
    "    # Most linked pages\n",
    "    print(\"Most linked internal pages:\")\n",
    "    target_counts = internal_links['target_url'].value_counts().head(10)\n",
    "    print(target_counts)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    target_counts.plot(kind='barh')\n",
    "    plt.xlabel('Number of Incoming Links')\n",
    "    plt.title(f'Top 10 Most Linked Pages - {ngo_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze external links\n",
    "if links:\n",
    "    external_links = links_df[links_df['link_type'] == 'external']\n",
    "    \n",
    "    # Extract domains from external links\n",
    "    from urllib.parse import urlparse\n",
    "    \n",
    "    external_links['domain'] = external_links['target_url'].apply(\n",
    "        lambda x: urlparse(x).netloc\n",
    "    )\n",
    "    \n",
    "    print(\"Top external domains linked to:\")\n",
    "    domain_counts = external_links['domain'].value_counts().head(15)\n",
    "    print(domain_counts)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    domain_counts.plot(kind='barh')\n",
    "    plt.xlabel('Number of Links')\n",
    "    plt.title(f'Top 15 External Domains - {ngo_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network graph\n",
    "if links:\n",
    "    # Build network from internal links only\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in internal_links.iterrows():\n",
    "        G.add_edge(row['source_url'], row['target_url'])\n",
    "    \n",
    "    print(f\"Network statistics:\")\n",
    "    print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"  Edges: {G.number_of_edges()}\")\n",
    "    print(f\"  Density: {nx.density(G):.4f}\")\n",
    "    \n",
    "    # Calculate degree centrality\n",
    "    in_degree = dict(G.in_degree())\n",
    "    out_degree = dict(G.out_degree())\n",
    "    \n",
    "    # Most central pages (by in-degree)\n",
    "    print(\"\\nMost central pages (by incoming links):\")\n",
    "    sorted_in_degree = sorted(in_degree.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for url, degree in sorted_in_degree:\n",
    "        print(f\"  {degree}: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document metadata\n",
    "def load_document_metadata(ngo_name):\n",
    "    \"\"\"Load metadata about downloaded documents.\"\"\"\n",
    "    ngo_path = data_dir / 'metadata' / ngo_name\n",
    "    sessions = sorted(ngo_path.glob('*'), reverse=True)\n",
    "    \n",
    "    if not sessions:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    latest_session = sessions[0]\n",
    "    metadata_file = latest_session / 'documents_metadata.jsonl'\n",
    "    \n",
    "    if not metadata_file.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Read JSONL file\n",
    "    docs = []\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            docs.append(json.loads(line))\n",
    "    \n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "docs_df = load_document_metadata(ngo_name)\n",
    "\n",
    "if not docs_df.empty:\n",
    "    print(f\"Total documents: {len(docs_df)}\")\n",
    "    print(f\"\\nDocument types:\")\n",
    "    print(docs_df['content_type'].value_counts())\n",
    "    \n",
    "    # Document sizes\n",
    "    print(f\"\\nDocument size statistics (bytes):\")\n",
    "    print(docs_df['size_bytes'].describe())\n",
    "    \n",
    "    # Display sample\n",
    "    docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-NGO Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for all NGOs\n",
    "all_ngo_stats = []\n",
    "\n",
    "for ngo_dir in ngo_dirs:\n",
    "    ngo_name = ngo_dir.name\n",
    "    _, _, session_info = load_ngo_data(ngo_name)\n",
    "    if session_info:\n",
    "        all_ngo_stats.append(session_info)\n",
    "\n",
    "# Create DataFrame\n",
    "ngo_stats_df = pd.DataFrame(all_ngo_stats)\n",
    "\n",
    "if not ngo_stats_df.empty:\n",
    "    print(\"Summary statistics across all NGOs:\")\n",
    "    print(ngo_stats_df[['n_pages', 'n_documents', 'n_links']].describe())\n",
    "    \n",
    "    # Display all NGOs\n",
    "    ngo_stats_df.sort_values('n_pages', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if not ngo_stats_df.empty:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Pages\n",
    "    ngo_stats_df.sort_values('n_pages', ascending=True).plot(\n",
    "        x='ngo_name', y='n_pages', kind='barh', ax=axes[0], legend=False\n",
    "    )\n",
    "    axes[0].set_title('Pages Scraped')\n",
    "    axes[0].set_xlabel('Number of Pages')\n",
    "    \n",
    "    # Documents\n",
    "    ngo_stats_df.sort_values('n_documents', ascending=True).plot(\n",
    "        x='ngo_name', y='n_documents', kind='barh', ax=axes[1], legend=False, color='orange'\n",
    "    )\n",
    "    axes[1].set_title('Documents Downloaded')\n",
    "    axes[1].set_xlabel('Number of Documents')\n",
    "    \n",
    "    # Links\n",
    "    ngo_stats_df.sort_values('n_links', ascending=True).plot(\n",
    "        x='ngo_name', y='n_links', kind='barh', ax=axes[2], legend=False, color='green'\n",
    "    )\n",
    "    axes[2].set_title('Links Extracted')\n",
    "    axes[2].set_xlabel('Number of Links')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inter-NGO Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build inter-NGO network based on external links\n",
    "# This shows which NGOs link to each other\n",
    "\n",
    "inter_ngo_edges = []\n",
    "\n",
    "# Load NGO domains\n",
    "ngo_config = pd.read_csv('../config/ngo_list.csv')\n",
    "ngo_domains = dict(zip(ngo_config['canonical_name'], ngo_config['website_domain']))\n",
    "\n",
    "# For each NGO, check external links to other NGOs\n",
    "for ngo_name in ngo_domains.keys():\n",
    "    links, _, _ = load_ngo_data(ngo_name)\n",
    "    if not links:\n",
    "        continue\n",
    "    \n",
    "    links_df = pd.DataFrame(links)\n",
    "    external = links_df[links_df['link_type'] == 'external']\n",
    "    \n",
    "    for _, row in external.iterrows():\n",
    "        target_url = row['target_url']\n",
    "        \n",
    "        # Check if this links to another NGO\n",
    "        for target_ngo, domain in ngo_domains.items():\n",
    "            if domain in target_url and target_ngo != ngo_name:\n",
    "                inter_ngo_edges.append({\n",
    "                    'source': ngo_name,\n",
    "                    'target': target_ngo\n",
    "                })\n",
    "                break\n",
    "\n",
    "if inter_ngo_edges:\n",
    "    inter_ngo_df = pd.DataFrame(inter_ngo_edges)\n",
    "    print(f\"Found {len(inter_ngo_df)} inter-NGO links\")\n",
    "    \n",
    "    # Count links between NGOs\n",
    "    link_counts = inter_ngo_df.groupby(['source', 'target']).size().reset_index(name='count')\n",
    "    print(\"\\nTop inter-NGO connections:\")\n",
    "    print(link_counts.sort_values('count', ascending=False).head(10))\n",
    "else:\n",
    "    print(\"No inter-NGO links found (need to scrape more NGOs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inter-NGO network\n",
    "if inter_ngo_edges:\n",
    "    # Create network\n",
    "    G_inter = nx.DiGraph()\n",
    "    for _, row in link_counts.iterrows():\n",
    "        G_inter.add_edge(row['source'], row['target'], weight=row['count'])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    pos = nx.spring_layout(G_inter, k=2, iterations=50)\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G_inter, pos, node_size=1000, node_color='lightblue', alpha=0.9)\n",
    "    \n",
    "    # Draw edges with varying thickness\n",
    "    edges = G_inter.edges()\n",
    "    weights = [G_inter[u][v]['weight'] for u, v in edges]\n",
    "    nx.draw_networkx_edges(G_inter, pos, width=[w*0.5 for w in weights], \n",
    "                          alpha=0.5, arrows=True, arrowsize=20)\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G_inter, pos, font_size=8)\n",
    "    \n",
    "    plt.title('Inter-NGO Link Network')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data for further analysis\n",
    "if not ngo_stats_df.empty:\n",
    "    ngo_stats_df.to_csv('../data/ngo_summary_statistics.csv', index=False)\n",
    "    print(\"Exported summary statistics to: data/ngo_summary_statistics.csv\")\n",
    "\n",
    "if inter_ngo_edges:\n",
    "    link_counts.to_csv('../data/inter_ngo_links.csv', index=False)\n",
    "    print(\"Exported inter-NGO links to: data/inter_ngo_links.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Content Analysis**: Extract and analyze text content from HTML pages\n",
    "2. **LLM Analysis**: Use LLMs to extract structured information about relationships\n",
    "3. **Comparison**: Compare with survey-based COMPON data\n",
    "4. **Temporal Analysis**: Track changes over multiple scraping sessions\n",
    "5. **Topic Modeling**: Identify main themes and topics across NGOs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
